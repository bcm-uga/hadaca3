---
title: "Detailed Results"
author: "Elise Amblard, Hugo Barbot, Florent Chuffart and Magali Richard"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    self_contained: true
    theme: cayman
    highlight: github
    toc: true
    # toc_float: true
    toc_depth: 3
    number_sections: true
    keep_tex: yes
---



```{r label = "header", echo = FALSE, eval = TRUE,include=FALSE}
# knitr::opts_chunk$set(collapse = TRUE, comment = "", echo = FALSE)

  requiered_packages = c("fmsb","circlize") #,"httpuv") #"catools") 

  installed_packages <- installed.packages( )
  for (package in requiered_packages ) {
      if ( !{ package %in% installed_packages } ) {
          print(x = paste("Installation of ", package, sep = "") )
          install.packages(
              pkgs = package
            , repos = "https://cloud.r-project.org"
          )
      } else {
          print(x = paste(package, " is installed.", sep = "") )
      }
      library(package, character.only = TRUE)
  }
  remove(list = c("installed_packages", "package") )
  

knitr::opts_chunk$set(collapse=TRUE, comment = "#>", fig.width=9, fig.height=6, eval=TRUE, echo=FALSE)#, results="hide")
options(knitr.duplicate.label = "allow")
```


```{r}

nb_datasets<- 4


# input =  "test_output"

all_prediction <- readRDS(paste0(input,.Platform$file.sep,"res",.Platform$file.sep,"prediction.rds"))

all_groundtruh = c()

all_scores = c()

for (dataset_name in 1:nb_datasets){
  ground_truth_file = paste0(input, .Platform$file.sep, "ref", .Platform$file.sep, "ground_truth_",toString(dataset_name),".rds")
  ground_truth =  readRDS(file = ground_truth_file )
  all_groundtruh[[dataset_name]] = ground_truth

  score_file = paste0(output, .Platform$file.sep, "scores_",toString(dataset_name),".rds")
  score =  readRDS(file = score_file )
  all_scores[[dataset_name]] = score

}

profiling <- readRDS(file = paste0(input, .Platform$file.sep, "res", .Platform$file.sep, "Rprof.rds") )


```

# Summaries 
## Summary of the prediction

### Heatmap of prediction

```{r Heatmap_pred}

for (dataset_name in 1:nb_datasets){

    prediction <- all_prediction[[dataset_name]]

  if (is.null(colnames(prediction))) {
    colnames(prediction) = paste0("Sample", format(1:ncol(prediction), digits = ceiling(log10(ncol(prediction)))) )
  }

  print(ComplexHeatmap::Heatmap(
    t(prediction),
    col = circlize::colorRamp2(seq(0, 1, length.out = 32), rev(heat.colors(32))),
    column_title = paste("Estimated proportion on dataset",dataset_name ) ,
                        #if (dataset_used == "smoothie") {"smoothies"} else {which(dataset_used == datasets)}),
    cluster_rows = FALSE,
    cluster_columns = FALSE,
    heatmap_legend_param = list(title = "Proportion"),
    column_names_rot = 45
  ))
}
```


### Boxplot of the absolute bias/error

```{r abs_bias}

for (dataset_name in 1:nb_datasets){

  prediction <- all_prediction[[dataset_name]]
  ground_truth <- all_groundtruh[[dataset_name]]

  abs_bias = abs(prediction - ground_truth)


  boxplot(
    t(abs_bias),
    bty = "n",
    col = "tan2",
    xlab = "Cell types",
    ylab = "Absolute bias",
    main =  paste("Absolute bias among each cell type",dataset_name) 
    )
  


}
```

Boxplot of absolute bias between estimated proportion and ground truth proportion $|\hat{\beta}_{prop} - \beta_{true\_prop}|$ for each cell type. 

## Summary of scores


- Global scores:


```{r scores_global_table,results='asis',message = FALSE,warning=FALSE}

for (dataset_name in 1:nb_datasets){

  score = all_scores[[dataset_name]]
  
  print(knitr::kable(score$baseline_estimation,caption = paste('Result table of the dataset',dataset_name)))

}
```


```{r scores_global_radarchart,message = FALSE}

for (dataset_name in 1:nb_datasets){

  score = all_scores[[dataset_name]]

  # We don't take the global score for the radar chart since it's a proxy of the area
  df_radarplot = as.data.frame(rbind(
    c(0,0,1,1,1), # the max value for each var
    c(0.5,0.5,0,0,0), # the min value for each var
    score$baseline_estimation[, -ncol(score$baseline_estimation)]
  ))
  # fmsb::radarchart(df_radarplot)
  fmsb::radarchart(df_radarplot, axistype=2 , 
                    # Performance
      title = paste("Score of each metric for on dataset", dataset_name ),
  
      #custom polygon
      pcol=rgb(0.2,0.5,0.5,0.9), pfcol=rgb(0.2,0.5,0.5,0.5), plwd=4,
  
      #custom the grid
      cglcol="grey", cglty=1, axislabcol="grey50", cglwd=0.8,
  
      #custom labels
      vlcex=0.8
      )
  

# Comment HB: the visualization by a radar/spider plot is intrinsecly linked to the aggregate score for defining the scope (min and max) for each variable.

}
```


- Scores per samples:
```{r scores_samples,message = FALSE}
for (dataset_name in 1:nb_datasets){

  score = all_scores[[dataset_name]]


df_score_samples = cbind(score$rmse_samples, score$mae_samples, 1-score$pearson_samples)
colnames(df_score_samples) = c("RMSE", "MAE", "1-Pearson")
invisible(boxplot(
  df_score_samples,
  bty = "n",
  col = "tan2",
  xlab = "",
  ylab = "",
  main = paste("Different scoring measures per sample, dataset",dataset_name)
))
}
```

Boxplot of the Root Mean Square Error, the Mean Absolute Error and one minus the pearson's correlation value, calculated for each sample. 



# Session Information

```{r, results="verbatim"}
sessionInfo()
```


